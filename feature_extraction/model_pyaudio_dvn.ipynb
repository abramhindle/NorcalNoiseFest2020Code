{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./drum_dbs/dk_data ./drum_dbs/dk_data.dill\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pytorch_models\n",
    "import imp\n",
    "import torchaudio as ta\n",
    "import torchaudio\n",
    "import torchvision as tv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import os, random\n",
    "import pandas as pd\n",
    "import mir_utils as miru\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_models import *\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pytorch_models as pm\n",
    "imp.reload(miru)\n",
    "imp.reload(pytorch_models)\n",
    "SR=44100\n",
    "#functions\n",
    "spec=torchaudio.functional.spectrogram\n",
    "#takes a billion years so commented\n",
    "audio_frames=miru.audioFrames(loadCache=True)\n",
    "\n",
    "def getMeanLength(x):\n",
    "    gl=x.apply(lambda z: len(z[\"audio\"]),axis=1)\n",
    "    print(gl.mean()/SR,gl.mean(),x[\"label\"].iloc[0])\n",
    "    \n",
    "# audio_frames.groupby(by=[\"label_num\"]).apply(lambda x:getMeanLength(x))\n",
    "# getMeanLength(audio_frames)\n",
    "\n",
    "\n",
    "def getRandAud():\n",
    "    classes=os.listdir(\"./dk_data/\")\n",
    "    rand_class=random.choice(classes)\n",
    "#     rand_class=\"synth_noise\"\n",
    "    rand_sample=random.choice(os.listdir(\"./dk_data/%s/\"%(rand_class,)))\n",
    "    return \"./dk_data/\"+rand_class+\"/\"+rand_sample\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# list(zip(uniques,range(0,len(uniques))))\n",
    "\n",
    "audio_frames=audio_frames.loc[~audio_frames[\"label\"].isin([\"shake\",\"guitar\",\"piano\"])]        \n",
    "#split the dataframe into 2 seperate ones:\n",
    "drum_frames=audio_frames.loc[~audio_frames[\"label\"].isin([\"synth_noise\",\"guitar\",\"piano\"])]\n",
    "#split the dataframe into 2 seperate ones:\n",
    "not_drum_frames=audio_frames.loc[audio_frames[\"label\"].isin([\"synth_noise\",\"guitar\",\"piano\"])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "clap             115\n",
      "hihat_closed     185\n",
      "hihat_open       230\n",
      "kick             614\n",
      "rim              102\n",
      "snare            700\n",
      "synth_noise     8000\n",
      "tom_high         144\n",
      "tom_low          165\n",
      "tom_mid          122\n",
      "Name: path, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#make two label series, one for drum vs not drum classification, one for drum type classification\n",
    "#add the series to the dataframe \n",
    "# print(drum_labels,not_drum_labels)\n",
    "\n",
    "#drum only classification\n",
    "# audio_frames[\"label_num\"].isin(not_drum_labels)\n",
    "dLabels, dUniques=pd.factorize(drum_frames[\"label\"].tolist())\n",
    "ndLabels, ndUniques=pd.factorize(not_drum_frames[\"label\"].tolist())\n",
    "\n",
    "drum_frames[\"label_num\"]=dLabels\n",
    "not_drum_frames[\"label_num\"]=ndLabels+len(dUniques)\n",
    "\n",
    "allU=np.concatenate([dUniques,ndUniques])\n",
    "lmap=list(zip(allU,range(len(allU))))\n",
    "\n",
    "#get weights for each group\n",
    "# x=drum_frames.groupby([\"label\"]).agg(\"count\")[\"path\"]\n",
    "y=audio_frames.groupby([\"label\"]).agg(\"count\")[\"path\"]\n",
    "print(y)\n",
    "weights=torch.tensor([1000/w for w in y.tolist()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dk_data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1fb86e518e5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#get env feats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mPATH_AUDIO\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetRandAud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0menvTrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmelEnv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMelScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_stft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-05dfb8b78447>\u001b[0m in \u001b[0;36mgetRandAud\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetRandAud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./dk_data/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mrand_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#     rand_class=\"synth_noise\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dk_data/'"
     ]
    }
   ],
   "source": [
    "#get env feats\n",
    "PATH_AUDIO=getRandAud()\n",
    "def envTrans(wf,num_mels=50):\n",
    "    melEnv=torchaudio.transforms.MelScale(n_mels=2*num_mels, sample_rate=SR, f_min=0.0, f_max=None, n_stft=None)\n",
    "    wf=wf[:,0:15000]\n",
    "    num_bins=wf[0].shape[0]//10\n",
    "\n",
    "    # spec(wf, pad, window, n_fft, hop_length, win_length, power)\n",
    "    win_length=num_bins\n",
    "    window=torch.tensor([1]*win_length)\n",
    "    s=spec(wf, 0, window, num_bins, win_length, win_length,2,normalized=False)\n",
    "    s=melEnv(s)\n",
    "\n",
    "    env=s.sum(axis=0).sum(axis=0)\n",
    "    env=env/env.abs().max()\n",
    "    env[torch.isnan(env)]=0\n",
    "\n",
    "    num_padding=torch.max(torch.tensor([num_mels+1-env.shape[0],0]))\n",
    "    env_vec=torch.cat([env.detach(),torch.zeros(num_padding)],dim=0)\n",
    "    return env_vec\n",
    "\n",
    "og_w, sample_rate = torchaudio.load(PATH_AUDIO)  # load tensor from file\n",
    "wf= ta.transforms.Resample(sample_rate,SR).forward(og_w)\n",
    "\n",
    "env_feats=envTrans(wf)\n",
    "plt.plot(env_feats)\n",
    "plt.show()\n",
    "print(PATH_AUDIO)\n",
    "Audio(wf,rate=SR, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get freq feats\n",
    "PATH_AUDIO=getRandAud()\n",
    "num_mels=100\n",
    "def freqTrans(wf,num_mels=num_mels):\n",
    "    ampT=torchaudio.transforms.AmplitudeToDB(stype='power', top_db=30)\n",
    "    mel=torchaudio.transforms.MelScale(n_mels=num_mels, sample_rate=SR, f_min=0.0, f_max=None, n_stft=None)\n",
    "    PATH_AUDIO=getRandAud()\n",
    "\n",
    "#     wf=wf[:,0:2000]\n",
    "    num_bins=wf[0].shape[0//2]\n",
    "    win_length=num_bins\n",
    "    window=torch.tensor([1]*win_length)\n",
    "    s=spec(wf, 100, window, num_bins, win_length, win_length,2,normalized=False)\n",
    "    s=mel(s)\n",
    "    s=ampT(s)\n",
    "    freq=s.sum(axis=0).sum(axis=1)\n",
    "    freq=freq/freq.abs().max()\n",
    "    freq[torch.isnan(freq)]=0\n",
    "    return freq.detach()\n",
    "\n",
    "og_w, sample_rate = torchaudio.load(PATH_AUDIO)  # load tensor from file\n",
    "wf= ta.transforms.Resample(sample_rate,SR).forward(og_w)\n",
    "\n",
    "freq_feats=freqTrans(wf,num_mels=num_mels)\n",
    "\n",
    "plt.plot(freq_feats)\n",
    "plt.show()\n",
    "print(freq_feats.shape)\n",
    "print(PATH_AUDIO)\n",
    "Audio(wf[0:15000],rate=SR, autoplay=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class audioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,audio_frame,root_dir, task=\"keep_all\",transform=None):\n",
    "        self.root_dir=root_dir\n",
    "        self.task=task\n",
    "        self.audio_frame=audio_frame\n",
    "        self.transform = transform\n",
    "        self.minLength=SR\n",
    "#         self.minLength=SR//4\n",
    "        self.frame_pruning()\n",
    "    def __len__(self):\n",
    "        return len(self.audio_frame)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        rows=self.audio_frame.iloc[idx]\n",
    "\n",
    "        signals,labels=rows[\"audio\"].tolist()[0:SR],rows[\"label_num\"].tolist()\n",
    "        signals,labels=torch.tensor(signals),torch.tensor(labels)\n",
    "        \n",
    "        nz=np.max((self.minLength-signals.shape[0],0))\n",
    "        signals=torch.cat([signals[0:self.minLength],torch.zeros(nz)],dim=0)\n",
    "\n",
    "        sound={\"signal\":signals,\"label\":labels}\n",
    "        \n",
    "        if self.transform:\n",
    "            sound = self.transform(sound)\n",
    "\n",
    "        return sound\n",
    "    \n",
    "    def frame_pruning(self):\n",
    "        #drum vs not drum classification:\n",
    "        drum_label_numbers=range(len(dUniques))\n",
    "        if self.task==\"dvn\":\n",
    "            self.audio_frame.loc[self.audio_frame[\"label_num\"].isin(drum_label_numbers),\"label_num\"]=0\n",
    "            self.audio_frame.loc[~self.audio_frame[\"label_num\"].isin(drum_label_numbers),\"label_num\"]=1\n",
    "        #drum type classification\n",
    "        if self.task==\"dvd\":\n",
    "            self.audio_frame=self.audio_frame.loc[self.audio_frame[\"label_num\"].isin(drum_label_numbers)]\n",
    "        if self.task==\"keep_all\":\n",
    "            pass\n",
    "            \n",
    "ds=audioDataset(pd.concat([drum_frames,not_drum_frames]),\"./\",task=\"dvn\",)\n",
    "idx=np.random.randint(0,len(ds),5)\n",
    "ds[np.random.randint(len(ds))]\n",
    "\n",
    "all_frames=pd.concat([drum_frames,not_drum_frames])\n",
    "train=all_frames.sample(frac=0.80,random_state=420) #random state is a seed value\n",
    "test=all_frames.drop(train.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#defining transformations\n",
    "class freqTrans(object):\n",
    "    def __init__(self,num_mels=50,SR=SR):\n",
    "        self.num_mels=num_mels\n",
    "        self.ampT=torchaudio.transforms.AmplitudeToDB(stype='power', top_db=30)\n",
    "        self.melF=torchaudio.transforms.MelScale(n_mels=self.num_mels, sample_rate=SR, f_min=30.0, f_max=None, n_stft=None)\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        wf,label=sample[\"signal\"],sample[\"label\"]\n",
    "        wf=wf.reshape(-1,len(wf))\n",
    "        sample_length=SR//4\n",
    "        wf=wf[:,0:24000]\n",
    "        num_bins=wf[0].shape[0]\n",
    "        win_length=num_bins\n",
    "        hop_step=sample_length//(self.num_mels)\n",
    "        window=torch.tensor([1]*win_length)\n",
    "        s=spec(wf, 100, window, num_bins, hop_step, win_length,2,normalized=False)\n",
    "        s=self.melF(s)\n",
    "        s=self.ampT(s)\n",
    "        freq=s.sum(axis=0).sum(axis=1)\n",
    "        freq=freq-freq.min()\n",
    "        freq=freq/freq.max()\n",
    "        freq[torch.isnan(freq)]=0\n",
    "        return {\"feats\":freq.detach(),\"label\":label}\n",
    "class specTrans(object):\n",
    "    def __init__(self,num_mels=50,SR=SR):\n",
    "        self.num_mels=num_mels\n",
    "        self.ampP=torchaudio.transforms.AmplitudeToDB(stype='power',top_db=60)\n",
    "        self.melP=torchaudio.transforms.MelScale(n_mels=self.num_mels, sample_rate=SR, f_min=30.0, f_max=15000.0, n_stft=None)\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        wf,label=sample[\"signal\"],sample[\"label\"]\n",
    "        wf=wf.reshape(-1,len(wf))\n",
    "        sample_length=SR\n",
    "\n",
    "        num_bins=wf[0].shape[0]\n",
    "        win_length=SR//17\n",
    "        hop_step=SR//19\n",
    "        window=torch.tensor([1]*win_length)\n",
    "        s=spec(wf, 0, window, num_bins, hop_step, win_length,2,normalized=False)\n",
    "        s=self.melP(s)\n",
    "        s=self.ampP(s)\n",
    "        s=s/s.abs().max()\n",
    "        freq=self.norm(s)\n",
    "        return {\"feats\":freq.detach(),\"label\":label}\n",
    "class envTrans(object):\n",
    "\n",
    "    def __init__(self,num_mels=10,SR=SR):\n",
    "        self.env_size=9\n",
    "        self.num_mels=num_mels\n",
    "        self.amp=torchaudio.transforms.AmplitudeToDB(stype='power', top_db=60)\n",
    "        self.melEnv=torchaudio.transforms.MelScale(n_mels=self.num_mels, sample_rate=SR, f_min=30.0, f_max=None, n_stft=None)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        wf,label=sample[\"signal\"],sample[\"label\"]\n",
    "\n",
    "        wf=wf.reshape(-1,len(wf))\n",
    "        sample_length=SR\n",
    "        num_bins=wf[0].shape[0]\n",
    "        win_length=SR//20\n",
    "        hop_step=SR//self.env_size\n",
    "        window=torch.tensor([1]*win_length)\n",
    "        s=spec(wf, 0, window,win_length, hop_step, win_length,2,normalized=False)\n",
    "        s=self.melEnv(s)\n",
    "        s=self.amp(s)\n",
    "        #normalizing\n",
    "        env=s.sum(axis=0).sum(axis=0)\n",
    "        env=env-env.min()\n",
    "        env=env/env.max()\n",
    "        env[torch.isnan(env)]=0\n",
    "\n",
    "        num_padding=torch.max(torch.tensor([self.env_size+1-env.shape[0],0]))\n",
    "        env_vec=torch.cat([env.detach(),torch.zeros(num_padding)],dim=0)\n",
    "        return {\"feats\":env_vec.detach(),\"label\":label}\n",
    "\n",
    "class feats_and_env(object):\n",
    "    def __init__(self,feat_mels=50,env_mels=1):\n",
    "        self.ft=freqTrans(num_mels=feat_mels)\n",
    "        self.et=envTrans(num_mels=env_mels)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "            combined_feats=torch.cat((self.ft(sample)[\"feats\"],self.et(sample)[\"feats\"]))\n",
    "            return {\"feats\":combined_feats,\"label\":sample[\"label\"]}\n",
    "        \n",
    "\n",
    "#Apply each of the above transforms on sample.\n",
    "fig = plt.figure(figsize=(20,4))\n",
    "# while True:\n",
    "# sample = ds[np.random.randint(len(ds))]\n",
    "#     print(sample[\"label\"].item())\n",
    "#     if sample[\"label\"].item()==1:\n",
    "#         break\n",
    "freq_train = audioDataset(train,\".\",task=\"dvd\")\n",
    "f_train_loader= DataLoader(freq_train, batch_size=1, num_workers=1)\n",
    "sample=iter(f_train_loader).next()\n",
    "s={\"signal\":sample[\"signal\"][0],\"label\":sample[\"label\"][0]}\n",
    "sample=s\n",
    "tfList=[freqTrans(50),envTrans(num_mels=10,SR=SR),specTrans(20)]\n",
    "for i, tsfrm in enumerate(tfList):\n",
    "\n",
    "    transformed_sample = tsfrm(sample)\n",
    "    ax = plt.subplot(1, 3, i + 1)\n",
    "    plt.tight_layout()\n",
    "#     ax.set_title(type(tsfrm).__name__)\n",
    "    ft=transformed_sample[\"feats\"]\n",
    "    if i==0:\n",
    "#         ff=ft\n",
    "        plt.title(\"Frequency Features\",fontsize=20)\n",
    "        plt.xlabel(\"Frequency Bin\",fontsize=20)\n",
    "        plt.ylabel(\"Scaled Magnitude\",fontsize=20)\n",
    "                   \n",
    "        plt.scatter(y=ft,x=range(0,len(ft)))\n",
    "    if i==1:\n",
    "#         ef=ft\n",
    "        plt.title(\"Envelope Features\",fontsize=20)\n",
    "        plt.xlabel(\"Time Step\",fontsize=20)\n",
    "        plt.ylabel(\"Scaled Magnitude\",fontsize=20)            \n",
    "        plt.scatter(y=ft,x=range(0,len(ft)))\n",
    "    if i==2:\n",
    "        sf=ft.detach().numpy()[0]\n",
    "        ft=ft.detach().numpy()[0]\n",
    "#         print(ft.shape,ft[0])\n",
    "        plt.title(\"Spectrum Features\",fontsize=20)\n",
    "\n",
    "        librosa.display.specshow(sf,cmap='gray_r',)\n",
    "        plt.xlabel(\"Time Step\",fontsize=20)\n",
    "        plt.ylabel(\"Magnitude of Bin\",fontsize=20)\n",
    "\n",
    "#         plt.savefig(\"./plots/ff3.pdf\",bbox_inches = \"tight\")\n",
    "# plt.show()\n",
    "print(lmap[transformed_sample[\"label\"].item()],transformed_sample[\"label\"],len(sample[\"signal\"]))\n",
    "\n",
    "\n",
    "Audio(sample[\"signal\"],rate=SR,autoplay=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC + env frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feats_and_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a114abb5758a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# combined_feats=torch.cat((tfList[0](sample)[\"feats\"],tfList[1](sample)[\"feats\"]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfne\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeats_and_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mall_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdrum_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnot_drum_frames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m420\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#random state is a seed value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feats_and_env' is not defined"
     ]
    }
   ],
   "source": [
    "#define a model for freq categorization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE, D_in,D_out =32,50,2\n",
    "# H1,H2,H3=100,50,10\n",
    "\n",
    "# combined_feats=torch.cat((tfList[0](sample)[\"feats\"],tfList[1](sample)[\"feats\"]))\n",
    "\n",
    "fne=feats_and_env()\n",
    "all_frames=pd.concat([drum_frames,not_drum_frames])\n",
    "train=all_frames.sample(frac=0.80,random_state=420) #random state is a seed value\n",
    "test=all_frames.drop(train.index)\n",
    "freq_train = audioDataset(train,\".\",task=\"dvn\", transform=fne)\n",
    "freq_test = audioDataset(test,\".\",task=\"dvn\", transform=fne)\n",
    "f_train_loader= DataLoader(freq_train, batch_size=BATCH_SIZE,shuffle=True, num_workers=12)\n",
    "f_test_loader= DataLoader(freq_test, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
    "\n",
    "import imp\n",
    "imp.reload(pm)  \n",
    "freq_model=pm.env_freq_Model(D_in=60,H1=30,H2=10,H3=10,device=device)\n",
    "\n",
    "freq_model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: 97 %\n",
      "0 100.4455857872963\n",
      "Accuracy on test: 97 %\n",
      "1 74.92084521055222\n",
      "Accuracy on test: 98 %\n",
      "2 73.2754927277565\n",
      "Accuracy on test: 97 %\n",
      "3 73.09104815125465\n",
      "Accuracy on test: 98 %\n",
      "4 72.5688279569149\n",
      "Accuracy on test: 98 %\n",
      "5 72.17083588242531\n",
      "Accuracy on test: 98 %\n",
      "6 71.93090409040451\n",
      "Accuracy on test: 98 %\n",
      "7 71.42613381147385\n",
      "Accuracy on test: 98 %\n",
      "8 71.46126911044121\n",
      "Accuracy on test: 98 %\n",
      "9 71.00226297974586\n",
      "Accuracy on test: 98 %\n",
      "10 70.73489436507225\n",
      "Accuracy on test: 98 %\n",
      "11 70.50995820760727\n",
      "Accuracy on test: 98 %\n",
      "12 70.62898716330528\n",
      "Accuracy on test: 97 %\n",
      "13 70.60067889094353\n",
      "Accuracy on test: 98 %\n",
      "14 70.73855385184288\n",
      "Accuracy on test: 98 %\n",
      "15 70.20294120907784\n",
      "Accuracy on test: 98 %\n",
      "16 69.96700832247734\n",
      "Accuracy on test: 98 %\n",
      "17 70.00137883424759\n",
      "Accuracy on test: 99 %\n",
      "18 69.99180626869202\n",
      "Accuracy on test: 98 %\n",
      "19 70.001907736063\n"
     ]
    }
   ],
   "source": [
    "weight=torch.tensor([0.80,0.2]).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(freq_model.parameters(), lr=learning_rate)\n",
    "epochs=20\n",
    "\n",
    "for i in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for j,x in enumerate(f_train_loader):\n",
    "\n",
    "        f,l=x[\"feats\"].to(device),x[\"label\"].to(device)\n",
    "  \n",
    "        optimizer.zero_grad()            \n",
    "        y_pred = freq_model(f)\n",
    "        loss = loss_fn(y_pred, l)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    for j,xt in enumerate(f_test_loader):\n",
    "        ft,lt=xt[\"feats\"].to(device),xt[\"label\"].to(device)\n",
    "        outputs = freq_model(ft) \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += lt.size(0)\n",
    "        correct += (predicted == lt).sum().item()\n",
    "        acc=(100 * correct / total)\n",
    "        test_loss=running_loss/train.shape[0]\n",
    "    if acc>80:\n",
    "            torch.save(freq_model.state_dict(),'./models/fc/freq_env_dvn_%d_%d.pt'%(acc,running_loss))\n",
    "\n",
    "    print('Accuracy on test: %d %%' % (acc))\n",
    "    print(i,running_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: 71 %\n",
      "0 67.91820725798607\n",
      "Accuracy on test: 71 %\n",
      "1 39.68742182850838\n",
      "Accuracy on test: 71 %\n",
      "2 38.98239782452583\n",
      "Accuracy on test: 71 %\n",
      "3 38.79981043934822\n",
      "Accuracy on test: 71 %\n",
      "4 38.55059000849724\n",
      "Accuracy on test: 89 %\n",
      "5 37.70563718676567\n",
      "Accuracy on test: 93 %\n",
      "6 36.80095052719116\n",
      "Accuracy on test: 92 %\n",
      "7 36.42999845743179\n",
      "Accuracy on test: 94 %\n",
      "8 36.27177149057388\n",
      "Accuracy on test: 93 %\n",
      "9 36.16971901059151\n",
      "Accuracy on test: 94 %\n",
      "10 36.11120891571045\n",
      "Accuracy on test: 94 %\n",
      "11 36.06236785650253\n",
      "Accuracy on test: 94 %\n",
      "12 35.99535968899727\n",
      "Accuracy on test: 94 %\n",
      "13 36.00020521879196\n",
      "Accuracy on test: 94 %\n",
      "14 35.91305032372475\n",
      "Accuracy on test: 94 %\n",
      "15 35.868190824985504\n",
      "Accuracy on test: 94 %\n",
      "16 35.937176048755646\n",
      "Accuracy on test: 94 %\n",
      "17 35.81636196374893\n",
      "Accuracy on test: 94 %\n",
      "18 35.79657202959061\n",
      "Accuracy on test: 94 %\n",
      "19 35.78626337647438\n",
      "Accuracy on test: 94 %\n",
      "20 35.76361536979675\n",
      "Accuracy on test: 94 %\n",
      "21 35.75030514597893\n",
      "Accuracy on test: 94 %\n",
      "22 35.75705015659332\n",
      "Accuracy on test: 94 %\n",
      "23 35.740578413009644\n",
      "Accuracy on test: 94 %\n",
      "24 35.72305378317833\n",
      "Accuracy on test: 95 %\n",
      "25 35.667985022068024\n",
      "Accuracy on test: 94 %\n",
      "26 35.7205314040184\n",
      "Accuracy on test: 94 %\n",
      "27 35.7884556055069\n",
      "Accuracy on test: 94 %\n",
      "28 35.67495679855347\n",
      "Accuracy on test: 94 %\n",
      "29 35.694855600595474\n"
     ]
    }
   ],
   "source": [
    "#using env features\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_env_feats=1\n",
    "BATCH_SIZE, D_in,D_out =64,num_env_feats+1,2\n",
    "H1,H2,H3,H4,H5=10,10,10,5,2\n",
    "\n",
    "et=tv.transforms.Compose([envTrans(num_env_feats)])\n",
    "\n",
    "train=all_frames.sample(frac=0.80,random_state=420) #random state is a seed value\n",
    "test=all_frames.drop(train.index)\n",
    "env_train = audioDataset(train,\".\",task=\"dvn\", transform=et)\n",
    "env_test = audioDataset(test,\".\",task=\"dvn\", transform=et)\n",
    "env_train_loader= DataLoader(env_train, batch_size=BATCH_SIZE,shuffle=True, num_workers=6)\n",
    "env_test_loader= DataLoader(env_test, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
    "import imp\n",
    "imp.reload(pm)  \n",
    "\n",
    "model_env = pm.env_Model(D_in=10,H1=10,H2=5,H3=10,H4=2,H5=10,device=device)\n",
    "# state='./models/fc/env_dvn_96_40.pt'\n",
    "# model_env.load_state_dict(torch.load(state))\n",
    "\n",
    "\n",
    "weights=torch.tensor([0.1,0.9]).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model_env.parameters(), lr=learning_rate)\n",
    "epochs=30\n",
    "\n",
    "for i in range(epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for j,x in enumerate(env_train_loader):\n",
    "        f,l=x[\"feats\"].to(device),x[\"label\"].to(device)\n",
    "        optimizer.zero_grad()            \n",
    "        y_pred = model_env(f)\n",
    "        loss = loss_fn(y_pred, l)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    for j,xt in enumerate(env_test_loader):\n",
    "\n",
    "        ft,lt=xt[\"feats\"].to(device),xt[\"label\"].to(device)\n",
    "        outputs = model_env(ft) \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += lt.size(0)\n",
    "        correct += (predicted == lt).sum().item()\n",
    "    \n",
    "    correctness=(100 * correct / total)\n",
    "    train_loss=running_loss\n",
    "    save=False\n",
    "    if correctness>90 or save==True:\n",
    "            torch.save(model_env.state_dict(),'./models/fc/env_dvn_%d_%d.pt'%(correctness,train_loss))\n",
    "\n",
    "    print('Accuracy on test: %d %%' % (correctness,))\n",
    "    print(i,train_loss)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC SPEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fbfc0abcf093>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspecTrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m420\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#random state is a seed value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpitch_data_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudioDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dvn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_frames' is not defined"
     ]
    }
   ],
   "source": [
    "#using whole spec features\n",
    "#define a model for freq categorization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pt=tv.transforms.Compose([specTrans(20)])\n",
    "\n",
    "train=all_frames.sample(frac=0.80,random_state=420) #random state is a seed value\n",
    "test=all_frames.drop(train.index)\n",
    "pitch_data_train = audioDataset(train,\".\",\"dvn\", transform=pt)\n",
    "pitch_data_test = audioDataset(test,\".\",task=\"dvn\", transform=pt)\n",
    "pitch_train_loader= DataLoader(pitch_data_train, batch_size=BATCH_SIZE,shuffle=True, num_workers=6)\n",
    "pitch_test_loader= DataLoader(pitch_data_test, batch_size=BATCH_SIZE,shuffle=True, num_workers=6)\n",
    "\n",
    "\n",
    "import imp\n",
    "imp.reload(pm)  \n",
    "pitch_model = pm.getFCSpecModel(D_in=20*20,H1=20,H2=10,H3=4)\n",
    "pitch_model.to(device)\n",
    "\n",
    "weights=torch.tensor([0.1,0.9]).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(pitch_model.parameters(), lr=learning_rate)\n",
    "epochs=30\n",
    "\n",
    "for i in range(epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for j,x in enumerate(pitch_train_loader):\n",
    "        f,l=x[\"feats\"].to(device),x[\"label\"].to(device)\n",
    "        f=torch.flatten(f, start_dim=1)\n",
    "        \n",
    "        optimizer.zero_grad()            \n",
    "        y_pred = pitch_model(f)\n",
    "    \n",
    "        loss = loss_fn(y_pred, l)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    for j,xt in enumerate(pitch_test_loader):\n",
    "\n",
    "        ft,lt=xt[\"feats\"].to(device),xt[\"label\"].to(device)\n",
    "        ft=torch.flatten(ft, start_dim=1)\n",
    "        outputs = pitch_model(ft) \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += lt.size(0)\n",
    "        correct += (predicted == lt).sum().item()\n",
    "    \n",
    "    correctness=(100 * correct / total)\n",
    "    train_loss=running_loss/BATCH_SIZE\n",
    "    save=False\n",
    "    if correctness>=90 or save==True:\n",
    "            torch.save(pitch_model.state_dict(),'./models/fc/spec_fc_dvn_%d.pt'%(correctness,))\n",
    "\n",
    "    print('Accuracy on test: %d ' % (correctness,))\n",
    "    print(i,train_loss)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-29d027cf407c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspecTrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m420\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#random state is a seed value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mpitch_data_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudioDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dvn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_frames' is not defined"
     ]
    }
   ],
   "source": [
    "#using whole spec features\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_pitch_feats=20*21\n",
    "BATCH_SIZE, D_in,D_out =8,num_pitch_feats+1,2\n",
    "H1,H2,H3,H4,H5=400,200,100,50,2\n",
    "\n",
    "\n",
    "\n",
    "pt=tv.transforms.Compose([specTrans(20)])\n",
    "\n",
    "train=all_frames.sample(frac=0.80,random_state=420) #random state is a seed value\n",
    "test=all_frames.drop(train.index)\n",
    "pitch_data_train = audioDataset(train,\".\",\"dvn\", transform=pt)\n",
    "pitch_data_test = audioDataset(test,\".\",task=\"dvn\", transform=pt)\n",
    "pitch_train_loader= DataLoader(pitch_data_train, batch_size=BATCH_SIZE,shuffle=True, num_workers=6)\n",
    "pitch_test_loader= DataLoader(pitch_data_test, batch_size=BATCH_SIZE,shuffle=True, num_workers=6)\n",
    "\n",
    "import pytorch_models as pm\n",
    "import imp\n",
    "imp.reload(pm)\n",
    "mt=\"cnn\"\n",
    "mt=\"clst\"\n",
    "if mt==\"clst\":\n",
    "    pitch_model = pm.CNNLSTM_dvn()\n",
    "#     state='./models/cnn/clst_dvn_98_306.pt'\n",
    "#     pitch_model.load_state_dict(torch.load(state))\n",
    "else:\n",
    "    pitch_model = pm.CNN_dvn()\n",
    "\n",
    "pitch_model.to(device)\n",
    "weights=torch.torch.tensor([0.1,0.9]).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(pitch_model.parameters(), lr=learning_rate)\n",
    "epochs=30\n",
    "\n",
    "for i in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    pitch_model.train()\n",
    "    for j,x in enumerate(pitch_train_loader):\n",
    "        f,l=x[\"feats\"].to(device),x[\"label\"].to(device)\n",
    "#         f=torch.flatten(f, start_dim=1)\n",
    "#         print(f.shape)\n",
    "#         print(f)\n",
    "        optimizer.zero_grad()            \n",
    "        y_pred = pitch_model(f)\n",
    "\n",
    "        loss = loss_fn(y_pred, l)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(loss.item())\n",
    "        running_loss+=loss.item()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    pitch_model.eval()\n",
    "    for j,xt in enumerate(pitch_test_loader):\n",
    "\n",
    "        ft,lt=xt[\"feats\"].to(device),xt[\"label\"].to(device)\n",
    "#         ft=torch.flatten(ft, start_dim=1)\n",
    "        outputs = pitch_model(ft) \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += lt.size(0)\n",
    "        correct += (predicted == lt).sum().item()\n",
    "    \n",
    "    correctness=(100 * correct / total)\n",
    "    train_loss=running_loss/BATCH_SIZE\n",
    "    save=False\n",
    "    if correctness>=90 or save==True:\n",
    "            torch.save(pitch_model.state_dict(),'./models/cnn/%s_dvn_%d_%d.pt'%(mt,correctness,running_loss))\n",
    "\n",
    "    print('loss %d, acc %d' % (running_loss,correctness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pitch_train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4c048bb6fe7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpitch_train_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pitch_train_loader' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
