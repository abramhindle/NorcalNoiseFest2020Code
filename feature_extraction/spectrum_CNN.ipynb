{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "EPOCHS = 3000\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.003\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"./cnn_data/train\"\n",
    "TEST_DATA_PATH = \"./cnn_data/test\"\n",
    "TRANSFORM_IMG = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=TRANSFORM_IMG)\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "test_data = torchvision.datasets.ImageFolder(root=TEST_DATA_PATH, transform=TRANSFORM_IMG)\n",
    "test_loader  = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader=test_loader\n",
    "\n",
    "classes=list(test_data.class_to_idx.keys())\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "#Training\n",
    "n_training_samples = 10000\n",
    "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
    "\n",
    "#Validation\n",
    "n_val_samples = 100\n",
    "val_sampler = SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n",
    "\n",
    "#Test\n",
    "n_test_samples = 1000\n",
    "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))\n",
    "\n",
    "\n",
    "def createLossAndOptimizer(net, learning_rate=0.001):\n",
    "    \n",
    "    #Loss function\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return(loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  rim noise snare  clap hihat_open  clap  kick   tom\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_net(torch.nn.Module):\n",
    "    \n",
    "    #Our batch shape for input x is (3, 128, 128)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN_net, self).__init__()\n",
    "        \n",
    "        #Input channels = 3, output channels = 18\n",
    "        self.conv1 = torch.nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        #4608 input features, 64 output features (see sizing flow below)\n",
    "        self.fc1 = torch.nn.Linear(18*64*64, 64)\n",
    "        \n",
    "        #64 input features, 10 output features for our 10 defined classes\n",
    "        self.fc2 = torch.nn.Linear(64, 19)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Computes the activation of the first convolution\n",
    "        #Size changes from (3, 32, 32) to (18, 32, 32)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        #Size changes from (18, 32, 32) to (18, 16, 16)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #Reshape data to input to the input layer of the neural net\n",
    "        #Size changes from (18, 16, 16) to (1, 4608)\n",
    "        #Recall that the -1 infers this dimension from the other given dimension\n",
    "        x = x.view(-1, 18*64*64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#io calculator\n",
    "def outputSize(in_size, kernel_size, stride, padding):\n",
    "    output = int((in_size - kernel_size + 2*(padding)) / stride) + 1\n",
    "    print(output)\n",
    "# outputSize(128*128*3,kernel_size=2, stride=2, padding=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get training data\n",
    "#     train_loader = get_train_loader(batch_size)\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Create our loss and optimizer functions\n",
    "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            #Get inputs\n",
    "#             inputs, labels = data[0], data[1]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            #Wrap them in a Variable object\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            running_loss += loss_size.data\n",
    "            total_train_loss += loss_size.data\n",
    "            \n",
    "            #Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            \n",
    "            #Wrap tensors in Variables\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            #Forward pass\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.data\n",
    "            \n",
    "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 8\n",
      "epochs= 1000\n",
      "learning_rate= 0.001\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 2.69 took: 0.83s\n",
      "Epoch 1, 20% \t train_loss: 2.07 took: 0.73s\n",
      "Epoch 1, 30% \t train_loss: 1.79 took: 0.74s\n",
      "Epoch 1, 40% \t train_loss: 1.58 took: 0.73s\n",
      "Epoch 1, 50% \t train_loss: 1.57 took: 0.73s\n",
      "Epoch 1, 60% \t train_loss: 1.54 took: 0.73s\n",
      "Epoch 1, 70% \t train_loss: 1.47 took: 0.73s\n",
      "Epoch 1, 80% \t train_loss: 1.43 took: 0.74s\n",
      "Epoch 1, 90% \t train_loss: 1.40 took: 0.73s\n",
      "Validation loss = 1.36\n",
      "Epoch 2, 10% \t train_loss: 1.31 took: 0.85s\n",
      "Epoch 2, 20% \t train_loss: 1.23 took: 0.74s\n",
      "Epoch 2, 30% \t train_loss: 1.24 took: 0.74s\n",
      "Epoch 2, 40% \t train_loss: 1.26 took: 0.74s\n",
      "Epoch 2, 50% \t train_loss: 1.27 took: 0.74s\n",
      "Epoch 2, 60% \t train_loss: 1.19 took: 0.74s\n",
      "Epoch 2, 70% \t train_loss: 1.22 took: 0.74s\n",
      "Epoch 2, 80% \t train_loss: 1.18 took: 0.74s\n",
      "Epoch 2, 90% \t train_loss: 1.20 took: 0.73s\n",
      "Validation loss = 1.22\n",
      "Epoch 3, 10% \t train_loss: 1.09 took: 0.83s\n",
      "Epoch 3, 20% \t train_loss: 1.10 took: 0.74s\n",
      "Epoch 3, 30% \t train_loss: 1.07 took: 0.74s\n",
      "Epoch 3, 40% \t train_loss: 1.14 took: 0.74s\n",
      "Epoch 3, 50% \t train_loss: 1.10 took: 0.74s\n",
      "Epoch 3, 60% \t train_loss: 1.09 took: 0.74s\n",
      "Epoch 3, 70% \t train_loss: 1.05 took: 0.74s\n",
      "Epoch 3, 80% \t train_loss: 1.07 took: 0.74s\n",
      "Epoch 3, 90% \t train_loss: 1.05 took: 0.74s\n",
      "Validation loss = 1.12\n",
      "Epoch 4, 10% \t train_loss: 0.94 took: 0.85s\n",
      "Epoch 4, 20% \t train_loss: 0.99 took: 0.74s\n",
      "Epoch 4, 30% \t train_loss: 0.96 took: 0.74s\n",
      "Epoch 4, 40% \t train_loss: 0.97 took: 0.73s\n",
      "Epoch 4, 50% \t train_loss: 0.99 took: 0.74s\n",
      "Epoch 4, 60% \t train_loss: 0.96 took: 0.74s\n",
      "Epoch 4, 70% \t train_loss: 0.98 took: 0.73s\n",
      "Epoch 4, 80% \t train_loss: 0.95 took: 0.73s\n",
      "Epoch 4, 90% \t train_loss: 0.88 took: 0.74s\n",
      "Validation loss = 1.16\n",
      "Epoch 5, 10% \t train_loss: 0.89 took: 0.84s\n",
      "Epoch 5, 20% \t train_loss: 0.89 took: 0.74s\n",
      "Epoch 5, 30% \t train_loss: 0.84 took: 0.74s\n",
      "Epoch 5, 40% \t train_loss: 0.81 took: 0.74s\n",
      "Epoch 5, 50% \t train_loss: 0.82 took: 0.74s\n",
      "Epoch 5, 60% \t train_loss: 0.88 took: 0.74s\n",
      "Epoch 5, 70% \t train_loss: 0.84 took: 0.73s\n",
      "Epoch 5, 80% \t train_loss: 0.82 took: 0.74s\n",
      "Epoch 5, 90% \t train_loss: 0.81 took: 0.74s\n",
      "Validation loss = 1.03\n",
      "Epoch 6, 10% \t train_loss: 0.73 took: 0.84s\n",
      "Epoch 6, 20% \t train_loss: 0.74 took: 0.74s\n",
      "Epoch 6, 30% \t train_loss: 0.72 took: 0.74s\n",
      "Epoch 6, 40% \t train_loss: 0.72 took: 0.73s\n",
      "Epoch 6, 50% \t train_loss: 0.71 took: 0.73s\n",
      "Epoch 6, 60% \t train_loss: 0.71 took: 0.73s\n",
      "Epoch 6, 70% \t train_loss: 0.73 took: 0.73s\n",
      "Epoch 6, 80% \t train_loss: 0.74 took: 0.73s\n",
      "Epoch 6, 90% \t train_loss: 0.73 took: 0.73s\n",
      "Validation loss = 1.07\n",
      "Epoch 7, 10% \t train_loss: 0.62 took: 0.84s\n",
      "Epoch 7, 20% \t train_loss: 0.59 took: 0.74s\n",
      "Epoch 7, 30% \t train_loss: 0.62 took: 0.74s\n",
      "Epoch 7, 40% \t train_loss: 0.61 took: 0.74s\n",
      "Epoch 7, 50% \t train_loss: 0.62 took: 0.74s\n",
      "Epoch 7, 60% \t train_loss: 0.59 took: 0.74s\n",
      "Epoch 7, 70% \t train_loss: 0.62 took: 0.74s\n",
      "Epoch 7, 80% \t train_loss: 0.66 took: 0.74s\n",
      "Epoch 7, 90% \t train_loss: 0.63 took: 0.74s\n",
      "Validation loss = 1.06\n",
      "Epoch 8, 10% \t train_loss: 0.51 took: 0.85s\n",
      "Epoch 8, 20% \t train_loss: 0.52 took: 0.74s\n",
      "Epoch 8, 30% \t train_loss: 0.53 took: 0.73s\n",
      "Epoch 8, 40% \t train_loss: 0.53 took: 0.74s\n",
      "Epoch 8, 50% \t train_loss: 0.52 took: 0.74s\n",
      "Epoch 8, 60% \t train_loss: 0.56 took: 0.74s\n",
      "Epoch 8, 70% \t train_loss: 0.55 took: 0.74s\n",
      "Epoch 8, 80% \t train_loss: 0.54 took: 0.74s\n",
      "Epoch 8, 90% \t train_loss: 0.54 took: 0.75s\n",
      "Validation loss = 1.10\n",
      "Epoch 9, 10% \t train_loss: 0.45 took: 0.84s\n",
      "Epoch 9, 20% \t train_loss: 0.44 took: 0.75s\n",
      "Epoch 9, 30% \t train_loss: 0.44 took: 0.74s\n",
      "Epoch 9, 40% \t train_loss: 0.43 took: 0.74s\n",
      "Epoch 9, 50% \t train_loss: 0.44 took: 0.73s\n",
      "Epoch 9, 60% \t train_loss: 0.44 took: 0.74s\n",
      "Epoch 9, 70% \t train_loss: 0.47 took: 0.74s\n",
      "Epoch 9, 80% \t train_loss: 0.51 took: 0.73s\n",
      "Epoch 9, 90% \t train_loss: 0.50 took: 0.73s\n",
      "Validation loss = 1.21\n",
      "Epoch 10, 10% \t train_loss: 0.38 took: 0.85s\n",
      "Epoch 10, 20% \t train_loss: 0.38 took: 0.74s\n",
      "Epoch 10, 30% \t train_loss: 0.39 took: 0.74s\n",
      "Epoch 10, 40% \t train_loss: 0.42 took: 0.74s\n",
      "Epoch 10, 50% \t train_loss: 0.40 took: 0.73s\n",
      "Epoch 10, 60% \t train_loss: 0.40 took: 0.73s\n",
      "Epoch 10, 70% \t train_loss: 0.39 took: 0.74s\n",
      "Epoch 10, 80% \t train_loss: 0.39 took: 0.74s\n",
      "Epoch 10, 90% \t train_loss: 0.40 took: 0.73s\n",
      "Validation loss = 1.23\n",
      "Epoch 11, 10% \t train_loss: 0.35 took: 0.84s\n",
      "Epoch 11, 20% \t train_loss: 0.32 took: 0.73s\n",
      "Epoch 11, 30% \t train_loss: 0.32 took: 0.73s\n",
      "Epoch 11, 40% \t train_loss: 0.33 took: 0.73s\n",
      "Epoch 11, 50% \t train_loss: 0.33 took: 0.73s\n",
      "Epoch 11, 60% \t train_loss: 0.33 took: 0.73s\n",
      "Epoch 11, 70% \t train_loss: 0.33 took: 0.73s\n",
      "Epoch 11, 80% \t train_loss: 0.34 took: 0.73s\n",
      "Epoch 11, 90% \t train_loss: 0.38 took: 0.73s\n",
      "Validation loss = 1.42\n",
      "Epoch 12, 10% \t train_loss: 0.33 took: 0.84s\n",
      "Epoch 12, 20% \t train_loss: 0.29 took: 0.74s\n",
      "Epoch 12, 30% \t train_loss: 0.29 took: 0.74s\n",
      "Epoch 12, 40% \t train_loss: 0.31 took: 0.74s\n",
      "Epoch 12, 50% \t train_loss: 0.31 took: 0.74s\n",
      "Epoch 12, 60% \t train_loss: 0.31 took: 0.73s\n",
      "Epoch 12, 70% \t train_loss: 0.31 took: 0.73s\n",
      "Epoch 12, 80% \t train_loss: 0.30 took: 0.74s\n",
      "Epoch 12, 90% \t train_loss: 0.34 took: 0.74s\n",
      "Validation loss = 1.27\n",
      "Epoch 13, 10% \t train_loss: 0.28 took: 0.85s\n",
      "Epoch 13, 20% \t train_loss: 0.27 took: 0.74s\n",
      "Epoch 13, 30% \t train_loss: 0.34 took: 0.75s\n",
      "Epoch 13, 40% \t train_loss: 0.26 took: 0.75s\n",
      "Epoch 13, 50% \t train_loss: 0.26 took: 0.74s\n",
      "Epoch 13, 60% \t train_loss: 0.26 took: 0.75s\n",
      "Epoch 13, 70% \t train_loss: 0.26 took: 0.74s\n",
      "Epoch 13, 80% \t train_loss: 0.28 took: 0.74s\n",
      "Epoch 13, 90% \t train_loss: 0.25 took: 0.74s\n",
      "Validation loss = 1.46\n",
      "Epoch 14, 10% \t train_loss: 0.27 took: 0.88s\n",
      "Epoch 14, 20% \t train_loss: 0.23 took: 0.74s\n",
      "Epoch 14, 30% \t train_loss: 0.27 took: 0.74s\n",
      "Epoch 14, 40% \t train_loss: 0.25 took: 0.73s\n",
      "Epoch 14, 50% \t train_loss: 0.26 took: 0.73s\n",
      "Epoch 14, 60% \t train_loss: 0.27 took: 0.73s\n",
      "Epoch 14, 70% \t train_loss: 0.31 took: 0.73s\n",
      "Epoch 14, 80% \t train_loss: 0.30 took: 0.73s\n",
      "Epoch 14, 90% \t train_loss: 0.29 took: 0.74s\n",
      "Validation loss = 1.60\n",
      "Epoch 15, 10% \t train_loss: 0.26 took: 0.84s\n",
      "Epoch 15, 20% \t train_loss: 0.25 took: 0.74s\n",
      "Epoch 15, 30% \t train_loss: 0.23 took: 0.75s\n",
      "Epoch 15, 40% \t train_loss: 0.23 took: 0.74s\n",
      "Epoch 15, 50% \t train_loss: 0.23 took: 0.74s\n",
      "Epoch 15, 60% \t train_loss: 0.21 took: 0.74s\n",
      "Epoch 15, 70% \t train_loss: 0.23 took: 0.74s\n",
      "Epoch 15, 80% \t train_loss: 0.29 took: 0.75s\n",
      "Epoch 15, 90% \t train_loss: 0.25 took: 0.74s\n",
      "Validation loss = 1.60\n",
      "Epoch 16, 10% \t train_loss: 0.22 took: 0.84s\n",
      "Epoch 16, 20% \t train_loss: 0.22 took: 0.74s\n",
      "Epoch 16, 30% \t train_loss: 0.21 took: 0.75s\n",
      "Epoch 16, 40% \t train_loss: 0.22 took: 0.74s\n",
      "Epoch 16, 50% \t train_loss: 0.20 took: 0.74s\n",
      "Epoch 16, 60% \t train_loss: 0.26 took: 0.74s\n",
      "Epoch 16, 70% \t train_loss: 0.26 took: 0.74s\n",
      "Epoch 16, 80% \t train_loss: 0.22 took: 0.74s\n",
      "Epoch 16, 90% \t train_loss: 0.23 took: 0.73s\n",
      "Validation loss = 1.71\n",
      "Epoch 17, 10% \t train_loss: 0.21 took: 0.84s\n",
      "Epoch 17, 20% \t train_loss: 0.24 took: 0.74s\n",
      "Epoch 17, 30% \t train_loss: 0.19 took: 0.74s\n",
      "Epoch 17, 40% \t train_loss: 0.24 took: 0.74s\n",
      "Epoch 17, 50% \t train_loss: 0.27 took: 0.74s\n",
      "Epoch 17, 60% \t train_loss: 0.25 took: 0.74s\n",
      "Epoch 17, 70% \t train_loss: 0.22 took: 0.74s\n",
      "Epoch 17, 80% \t train_loss: 0.21 took: 0.73s\n",
      "Epoch 17, 90% \t train_loss: 0.21 took: 0.73s\n",
      "Validation loss = 1.80\n",
      "Epoch 18, 10% \t train_loss: 0.21 took: 0.85s\n",
      "Epoch 18, 20% \t train_loss: 0.19 took: 0.74s\n",
      "Epoch 18, 30% \t train_loss: 0.21 took: 0.75s\n",
      "Epoch 18, 40% \t train_loss: 0.19 took: 0.74s\n",
      "Epoch 18, 50% \t train_loss: 0.19 took: 0.74s\n",
      "Epoch 18, 60% \t train_loss: 0.19 took: 0.74s\n",
      "Epoch 18, 70% \t train_loss: 0.21 took: 0.74s\n",
      "Epoch 18, 80% \t train_loss: 0.22 took: 0.74s\n",
      "Epoch 18, 90% \t train_loss: 0.22 took: 0.74s\n",
      "Validation loss = 1.71\n",
      "Epoch 19, 10% \t train_loss: 0.24 took: 0.84s\n",
      "Epoch 19, 20% \t train_loss: 0.21 took: 0.73s\n",
      "Epoch 19, 30% \t train_loss: 0.20 took: 0.74s\n",
      "Epoch 19, 40% \t train_loss: 0.20 took: 0.74s\n",
      "Epoch 19, 50% \t train_loss: 0.20 took: 0.73s\n",
      "Epoch 19, 60% \t train_loss: 0.23 took: 0.74s\n",
      "Epoch 19, 70% \t train_loss: 0.18 took: 0.74s\n",
      "Epoch 19, 80% \t train_loss: 0.18 took: 0.73s\n",
      "Epoch 19, 90% \t train_loss: 0.23 took: 0.73s\n",
      "Validation loss = 1.66\n",
      "Epoch 20, 10% \t train_loss: 0.21 took: 0.84s\n",
      "Epoch 20, 20% \t train_loss: 0.20 took: 0.73s\n",
      "Epoch 20, 30% \t train_loss: 0.22 took: 0.74s\n",
      "Epoch 20, 40% \t train_loss: 0.19 took: 0.73s\n",
      "Epoch 20, 50% \t train_loss: 0.18 took: 0.73s\n",
      "Epoch 20, 60% \t train_loss: 0.19 took: 0.73s\n",
      "Epoch 20, 70% \t train_loss: 0.23 took: 0.73s\n",
      "Epoch 20, 80% \t train_loss: 0.24 took: 0.73s\n",
      "Epoch 20, 90% \t train_loss: 0.18 took: 0.73s\n",
      "Validation loss = 1.79\n",
      "Epoch 21, 10% \t train_loss: 0.22 took: 0.84s\n",
      "Epoch 21, 20% \t train_loss: 0.22 took: 0.74s\n",
      "Epoch 21, 30% \t train_loss: 0.20 took: 0.74s\n",
      "Epoch 21, 40% \t train_loss: 0.21 took: 0.74s\n",
      "Epoch 21, 50% \t train_loss: 0.21 took: 0.74s\n",
      "Epoch 21, 60% \t train_loss: 0.22 took: 0.74s\n",
      "Epoch 21, 70% \t train_loss: 0.19 took: 0.74s\n",
      "Epoch 21, 80% \t train_loss: 0.20 took: 0.74s\n",
      "Epoch 21, 90% \t train_loss: 0.16 took: 0.74s\n",
      "Validation loss = 1.76\n",
      "Epoch 22, 10% \t train_loss: 0.20 took: 0.85s\n",
      "Epoch 22, 20% \t train_loss: 0.23 took: 0.74s\n",
      "Epoch 22, 30% \t train_loss: 0.21 took: 0.74s\n",
      "Epoch 22, 40% \t train_loss: 0.20 took: 0.73s\n",
      "Epoch 22, 50% \t train_loss: 0.18 took: 0.73s\n",
      "Epoch 22, 60% \t train_loss: 0.18 took: 0.74s\n",
      "Epoch 22, 70% \t train_loss: 0.19 took: 0.74s\n",
      "Epoch 22, 80% \t train_loss: 0.17 took: 0.75s\n",
      "Epoch 22, 90% \t train_loss: 0.24 took: 0.74s\n",
      "Validation loss = 1.82\n",
      "Epoch 23, 10% \t train_loss: 0.19 took: 0.84s\n",
      "Epoch 23, 20% \t train_loss: 0.16 took: 0.74s\n",
      "Epoch 23, 30% \t train_loss: 0.16 took: 0.74s\n",
      "Epoch 23, 40% \t train_loss: 0.20 took: 0.74s\n",
      "Epoch 23, 50% \t train_loss: 0.19 took: 0.74s\n",
      "Epoch 23, 60% \t train_loss: 0.17 took: 0.74s\n",
      "Epoch 23, 70% \t train_loss: 0.19 took: 0.73s\n",
      "Epoch 23, 80% \t train_loss: 0.20 took: 0.74s\n",
      "Epoch 23, 90% \t train_loss: 0.18 took: 0.74s\n",
      "Validation loss = 2.09\n",
      "Epoch 24, 10% \t train_loss: 0.17 took: 0.84s\n",
      "Epoch 24, 20% \t train_loss: 0.21 took: 0.73s\n",
      "Epoch 24, 30% \t train_loss: 0.17 took: 0.73s\n",
      "Epoch 24, 40% \t train_loss: 0.21 took: 0.74s\n",
      "Epoch 24, 50% \t train_loss: 0.18 took: 0.73s\n",
      "Epoch 24, 60% \t train_loss: 0.21 took: 0.74s\n",
      "Epoch 24, 70% \t train_loss: 0.17 took: 0.73s\n",
      "Epoch 24, 80% \t train_loss: 0.19 took: 0.74s\n",
      "Epoch 24, 90% \t train_loss: 0.15 took: 0.74s\n",
      "Validation loss = 2.17\n",
      "Epoch 25, 10% \t train_loss: 0.20 took: 0.84s\n",
      "Epoch 25, 20% \t train_loss: 0.18 took: 0.75s\n",
      "Epoch 25, 30% \t train_loss: 0.17 took: 0.75s\n",
      "Epoch 25, 40% \t train_loss: 0.17 took: 0.74s\n",
      "Epoch 25, 50% \t train_loss: 0.20 took: 0.74s\n",
      "Epoch 25, 60% \t train_loss: 0.17 took: 0.74s\n",
      "Epoch 25, 70% \t train_loss: 0.21 took: 0.74s\n",
      "Epoch 25, 80% \t train_loss: 0.18 took: 0.73s\n",
      "Epoch 25, 90% \t train_loss: 0.21 took: 0.73s\n",
      "Validation loss = 1.99\n",
      "Epoch 26, 10% \t train_loss: 0.16 took: 0.84s\n",
      "Epoch 26, 20% \t train_loss: 0.19 took: 0.74s\n",
      "Epoch 26, 30% \t train_loss: 0.19 took: 0.74s\n",
      "Epoch 26, 40% \t train_loss: 0.19 took: 0.74s\n",
      "Epoch 26, 50% \t train_loss: 0.18 took: 0.74s\n",
      "Epoch 26, 60% \t train_loss: 0.17 took: 0.74s\n",
      "Epoch 26, 70% \t train_loss: 0.18 took: 0.73s\n",
      "Epoch 26, 80% \t train_loss: 0.21 took: 0.73s\n",
      "Epoch 26, 90% \t train_loss: 0.21 took: 0.73s\n",
      "Validation loss = 2.03\n",
      "Epoch 27, 10% \t train_loss: 0.15 took: 0.85s\n",
      "Epoch 27, 20% \t train_loss: 0.17 took: 0.74s\n",
      "Epoch 27, 30% \t train_loss: 0.19 took: 0.75s\n",
      "Epoch 27, 40% \t train_loss: 0.17 took: 0.75s\n",
      "Epoch 27, 50% \t train_loss: 0.18 took: 0.74s\n",
      "Epoch 27, 60% \t train_loss: 0.17 took: 0.74s\n",
      "Epoch 27, 70% \t train_loss: 0.17 took: 0.74s\n",
      "Epoch 27, 80% \t train_loss: 0.20 took: 0.74s\n",
      "Epoch 27, 90% \t train_loss: 0.18 took: 0.73s\n",
      "Validation loss = 2.10\n",
      "Epoch 28, 10% \t train_loss: 0.14 took: 0.83s\n",
      "Epoch 28, 20% \t train_loss: 0.13 took: 0.74s\n",
      "Epoch 28, 30% \t train_loss: 0.19 took: 0.73s\n",
      "Epoch 28, 40% \t train_loss: 0.19 took: 0.73s\n",
      "Epoch 28, 50% \t train_loss: 0.19 took: 0.73s\n",
      "Epoch 28, 60% \t train_loss: 0.18 took: 0.73s\n",
      "Epoch 28, 70% \t train_loss: 0.19 took: 0.73s\n",
      "Epoch 28, 80% \t train_loss: 0.17 took: 0.73s\n",
      "Epoch 28, 90% \t train_loss: 0.22 took: 0.73s\n",
      "Validation loss = 2.13\n",
      "Epoch 29, 10% \t train_loss: 0.15 took: 0.84s\n",
      "Epoch 29, 20% \t train_loss: 0.17 took: 0.74s\n",
      "Epoch 29, 30% \t train_loss: 0.23 took: 0.74s\n",
      "Epoch 29, 40% \t train_loss: 0.17 took: 0.75s\n",
      "Epoch 29, 50% \t train_loss: 0.15 took: 0.74s\n",
      "Epoch 29, 60% \t train_loss: 0.20 took: 0.73s\n",
      "Epoch 29, 70% \t train_loss: 0.23 took: 0.73s\n",
      "Epoch 29, 80% \t train_loss: 0.15 took: 0.73s\n",
      "Epoch 29, 90% \t train_loss: 0.16 took: 0.73s\n",
      "Validation loss = 2.40\n",
      "Epoch 30, 10% \t train_loss: 0.18 took: 0.83s\n",
      "Epoch 30, 20% \t train_loss: 0.17 took: 0.73s\n",
      "Epoch 30, 30% \t train_loss: 0.17 took: 0.73s\n",
      "Epoch 30, 40% \t train_loss: 0.18 took: 0.73s\n",
      "Epoch 30, 50% \t train_loss: 0.17 took: 0.73s\n",
      "Epoch 30, 60% \t train_loss: 0.19 took: 0.73s\n",
      "Epoch 30, 70% \t train_loss: 0.14 took: 0.73s\n",
      "Epoch 30, 80% \t train_loss: 0.19 took: 0.73s\n",
      "Epoch 30, 90% \t train_loss: 0.17 took: 0.73s\n",
      "Validation loss = 2.48\n",
      "Epoch 31, 10% \t train_loss: 0.17 took: 0.84s\n",
      "Epoch 31, 20% \t train_loss: 0.17 took: 0.74s\n",
      "Epoch 31, 30% \t train_loss: 0.14 took: 0.74s\n",
      "Epoch 31, 40% \t train_loss: 0.21 took: 0.74s\n",
      "Epoch 31, 50% \t train_loss: 0.16 took: 0.73s\n",
      "Epoch 31, 60% \t train_loss: 0.14 took: 0.73s\n",
      "Epoch 31, 70% \t train_loss: 0.16 took: 0.73s\n",
      "Epoch 31, 80% \t train_loss: 0.18 took: 0.73s\n",
      "Epoch 31, 90% \t train_loss: 0.16 took: 0.73s\n",
      "Validation loss = 2.69\n",
      "Epoch 32, 10% \t train_loss: 0.17 took: 0.84s\n",
      "Epoch 32, 20% \t train_loss: 0.18 took: 0.75s\n",
      "Epoch 32, 30% \t train_loss: 0.23 took: 0.75s\n",
      "Epoch 32, 40% \t train_loss: 0.16 took: 0.74s\n",
      "Epoch 32, 50% \t train_loss: 0.16 took: 0.74s\n",
      "Epoch 32, 60% \t train_loss: 0.16 took: 0.74s\n",
      "Epoch 32, 70% \t train_loss: 0.14 took: 0.74s\n",
      "Epoch 32, 80% \t train_loss: 0.17 took: 0.74s\n",
      "Epoch 32, 90% \t train_loss: 0.19 took: 0.74s\n",
      "Validation loss = 2.62\n",
      "Epoch 33, 10% \t train_loss: 0.18 took: 0.84s\n",
      "Epoch 33, 20% \t train_loss: 0.16 took: 0.74s\n",
      "Epoch 33, 30% \t train_loss: 0.25 took: 0.74s\n",
      "Epoch 33, 40% \t train_loss: 0.16 took: 0.74s\n",
      "Epoch 33, 50% \t train_loss: 0.16 took: 0.74s\n",
      "Epoch 33, 60% \t train_loss: 0.15 took: 0.74s\n",
      "Epoch 33, 70% \t train_loss: 0.18 took: 0.74s\n",
      "Epoch 33, 80% \t train_loss: 0.19 took: 0.74s\n",
      "Epoch 33, 90% \t train_loss: 0.18 took: 0.74s\n",
      "Validation loss = 2.45\n",
      "Epoch 34, 10% \t train_loss: 0.13 took: 0.84s\n",
      "Epoch 34, 20% \t train_loss: 0.16 took: 0.74s\n",
      "Epoch 34, 30% \t train_loss: 0.19 took: 0.75s\n",
      "Epoch 34, 40% \t train_loss: 0.16 took: 0.74s\n",
      "Epoch 34, 50% \t train_loss: 0.19 took: 0.74s\n",
      "Epoch 34, 60% \t train_loss: 0.16 took: 0.74s\n",
      "Epoch 34, 70% \t train_loss: 0.14 took: 0.74s\n",
      "Epoch 34, 80% \t train_loss: 0.21 took: 0.74s\n",
      "Epoch 34, 90% \t train_loss: 0.17 took: 0.74s\n",
      "Validation loss = 2.51\n",
      "Epoch 35, 10% \t train_loss: 0.18 took: 0.83s\n"
     ]
    }
   ],
   "source": [
    "CNN = CNN_net()\n",
    "CNN.to(device)\n",
    "trainNet(CNN, batch_size=BATCH_SIZE, n_epochs=1000, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './1000epoch.pth'\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(BATCH_SIZE)))\n",
    "\n",
    "dimg=images.to(device)\n",
    "outputs=CNN(dimg)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted:   ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
