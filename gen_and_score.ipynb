{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating then ranking audio\n",
    "- load the CNN model (or some other ranker) from feature_extraction \n",
    "- genereate parameters and its corresponding audio \"randomly\"\n",
    "- rank the parameters using the ranker\n",
    "- save the parameters that made the sound along with the rankings to some dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from feature_extraction import CNN_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "# generation imports\n",
    "from pippi.soundbuffer import SoundBuffer\n",
    "from pippi import dsp,fx\n",
    "import helpers as hp\n",
    "\n",
    "import param_generation as pg\n",
    "import _pickle as pickle\n",
    "from IPython.display import Audio\n",
    "from feature_extraction import mir_utils as mu\n",
    "###\n",
    "import torch.utils.data as utils\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "###\n",
    "import scipy.stats as ss\n",
    "sr=44100\n",
    "stack_size=3\n",
    "BATCH_SIZE=1\n",
    "classes=['clap', 'guitar',\n",
    "         'hat', 'kick', 'noise',\n",
    "         'piano', 'rim', 'shake', 'snare', 'synth','tom', 'voc']\n",
    "\n",
    "cDict={v:i for i,v in enumerate(classes)}\n",
    "\n",
    "# function to show an image\n",
    "def imshow(img):\n",
    "    img = img      # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\"\n",
    "s=torch.load(\"feature_extraction/models/model-4-18.states\")\n",
    "cnn = CNN_utils.CNN_net()\n",
    "cnn.load_state_dict(s[\"model_state_dict\"])\n",
    "cnn.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of how to make an audio stack, get audio, make (mel_spectrum) image for that audio\n",
    "out,params=hp.stackMaker(stack_size)\n",
    "a=hp.memToAud(out)\n",
    "# get the image for that audio\n",
    "im=mu.audToImage(a,128)\n",
    "im=-1*librosa.util.normalize(im)\n",
    "plt.imshow(im)\n",
    "Audio(a,rate=sr, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classesR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cb6507540a90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrowDict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrowDict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmakeRowSlow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-cb6507540a90>\u001b[0m in \u001b[0;36mmakeRowSlow\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mscore_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#ranks based on score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mranks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassesR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrankdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mrank_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_ranked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mranks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_dict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrowDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparamToDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classesR' is not defined"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# make a row of data and show what's going on\n",
    "def makeRowSlow():\n",
    "    ## function that makes a row of parameters and the scores for the parameters \n",
    "    ## this row can then be added to a dataframe/csv file etc\n",
    "    out,params=hp.stackMaker(3)\n",
    "    a=hp.memToAud(out)\n",
    "    # get the image for that audio\n",
    "    im=mu.audToImage(a,128)\n",
    "    z=librosa.util.normalize(im)\n",
    "    t= transforms.Compose(\n",
    "        [\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    #normalize array->pilform ->apply transoforms,\n",
    "    z=(((z - z.min()) / (z.max() - z.min())) * 255.9).astype(np.uint8)\n",
    "    zi=Image.fromarray(z)\n",
    "    z=t(zi)\n",
    "    images=z.reshape([1,1,128,128])\n",
    "    dimg=images.to(device)\n",
    "    outputs=cnn(dimg)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    o=outputs.cpu().detach().numpy()[0]\n",
    "    o_norm=o-min(o)\n",
    "    o_norm=o_norm/sum(o_norm)\n",
    "    score_dict=dict(zip(classes,o_norm))\n",
    "    #ranks based on score\n",
    "    ranks=1+len(classesR)-ss.rankdata(o_norm) \n",
    "    rank_dict=dict(zip(classes_ranked,ranks))\n",
    "    df=pd.concat([pd.DataFrame.from_dict([rank_dict]),pd.DataFrame.from_dict([rowDict]),hp.paramToDF(params)],axis=1) \n",
    "    \n",
    "    imshow(torchvision.utils.make_grid(images))\n",
    "    print(sorted(rowDict.items(), key=lambda x: x[1],reverse=True))\n",
    "\n",
    "    return a,rowDict,params,df\n",
    "a,rowDict,params,z=makeRowSlow()\n",
    "\n",
    "Audio(a,rate=sr, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### here's what a row looks like, next print rows like this without the headers so we can redirect it to a csv/text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clap_rank</th>\n",
       "      <th>guitar_rank</th>\n",
       "      <th>hat_rank</th>\n",
       "      <th>kick_rank</th>\n",
       "      <th>noise_rank</th>\n",
       "      <th>piano_rank</th>\n",
       "      <th>rim_rank</th>\n",
       "      <th>shake_rank</th>\n",
       "      <th>snare_rank</th>\n",
       "      <th>synth_rank</th>\n",
       "      <th>...</th>\n",
       "      <th>bpCutLow_0</th>\n",
       "      <th>bpCutHigh_0</th>\n",
       "      <th>bpOrder_0</th>\n",
       "      <th>length_0</th>\n",
       "      <th>start_0</th>\n",
       "      <th>pitch0_0</th>\n",
       "      <th>pitch1_0</th>\n",
       "      <th>pitch2_0</th>\n",
       "      <th>pitch3_0</th>\n",
       "      <th>pitch4_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>294.157295</td>\n",
       "      <td>4219.69493</td>\n",
       "      <td>2</td>\n",
       "      <td>0.730297</td>\n",
       "      <td>0.089901</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   clap_rank  guitar_rank  hat_rank  kick_rank  noise_rank  piano_rank  \\\n",
       "0       11.0          2.0       4.0        8.0         3.0         6.0   \n",
       "\n",
       "   rim_rank  shake_rank  snare_rank  synth_rank  ...  bpCutLow_0  bpCutHigh_0  \\\n",
       "0      10.0        12.0         9.0         1.0  ...  294.157295   4219.69493   \n",
       "\n",
       "   bpOrder_0  length_0   start_0  pitch0_0  pitch1_0  pitch2_0  pitch3_0  \\\n",
       "0          2  0.730297  0.089901       201       201       201       201   \n",
       "\n",
       "   pitch4_0  \n",
       "0       201  \n",
       "\n",
       "[1 rows x 45 columns]"
      ]
     },
     "execution_count": 980,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a lot of rows, this can be run as its own script if you want to do multi-processing\n",
    "\n",
    "for i in range(10):\n",
    "    ## function that makes a row of parameters and the scores for the parameters \n",
    "    ## this row can then be added to a dataframe/csv file etc\n",
    "    out,params=hp.stackMaker(1)\n",
    "    a=hp.memToAud(out)\n",
    "    \n",
    "    # get the image for that audio\n",
    "    \n",
    "    try:\n",
    "        im=mu.audToImage(a,128)\n",
    "    except:\n",
    "        pass\n",
    "    z=librosa.util.normalize(im)\n",
    "    t= transforms.Compose(\n",
    "        [\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    #normalize array->pilform ->apply transoforms,\n",
    "    z=(((z - z.min()) / (z.max() - z.min())) * 255.9).astype(np.uint8)\n",
    "    zi=Image.fromarray(z)\n",
    "    z=t(zi)\n",
    "    images=z.reshape([1,1,128,128])\n",
    "\n",
    "    dimg=images.to(device)\n",
    "    outputs=cnn(dimg)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    o=outputs.cpu().detach().numpy()[0]\n",
    "    o_norm=o-min(o)\n",
    "    o_norm=o_norm/sum(o_norm)\n",
    "    score_dict=dict(zip(classes,o_norm))\n",
    "    #ranks based on score\n",
    "    ranks=1+len(classes_ranked)-ss.rankdata(o_norm) \n",
    "    rank_dict=dict(zip(classes_ranked,ranks))\n",
    "    df=pd.concat([pd.DataFrame.from_dict([rank_dict]),pd.DataFrame.from_dict([score_dict]),hp.paramToDF(params)],axis=1)    \n",
    "    x=df.to_string(header=False,\n",
    "                  index=False,\n",
    "                  index_names=False).split('\\n')\n",
    "    vals = [','.join(ele.split()) for ele in x]\n",
    "    \n",
    "#     print(vals[0])\n",
    "    with open (\"test.txt\",\"a\") as t:\n",
    "        t.write(vals[0]+\"\\n\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
