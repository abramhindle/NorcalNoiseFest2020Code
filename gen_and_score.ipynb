{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating then ranking audio\n",
    "- load the CNN model (or some other ranker) from feature_extraction \n",
    "- genereate parameters and its corresponding audio \"randomly\"\n",
    "- rank the parameters using the ranker\n",
    "- save the parameters that made the sound along with the rankings to some dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generation imports\n",
    "from pippi.soundbuffer import SoundBuffer\n",
    "from pippi import dsp,fx\n",
    "import param_generation as pg\n",
    "import _pickle as pickle\n",
    "from IPython.display import Audio\n",
    "from feature_extraction.mir_utils import *\n",
    "###\n",
    "import torch.utils.data as utils\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "###\n",
    "import scipy.stats as ss\n",
    "import common_vars as comv\n",
    "import imp\n",
    "import librosa\n",
    "import librosa.display\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import imp\n",
    "import helpers\n",
    "imp.reload(helpers)\n",
    "imp.reload(comv)\n",
    "\n",
    "from feature_extraction import pytorch_models as tm\n",
    "imp.reload(tm)\n",
    "from helpers import *\n",
    "\n",
    "from common_vars import sr\n",
    "SR=sr\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "stack_size=3\n",
    "BATCH_SIZE=1\n",
    "NUM_BINS=100\n",
    "\n",
    "classes=comv.classes\n",
    "classes_ranked=comv.classes_ranked\n",
    "cDict={v:i for i,v in enumerate(classes)}\n",
    "\n",
    "\n",
    "# function to show an image\n",
    "def imshow(img):\n",
    "    img = img      # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 120\n",
    "seq_dim=100\n",
    "output_size = 5\n",
    "hidden_dim = 1000\n",
    "n_layers = 1\n",
    "\n",
    "\n",
    "#env model stuff\n",
    "H1,H2,H3,H4,H5=31,15,10,5,2\n",
    "env_model_dvn = tm.env_Model(D_in=21,H1=4,H2=2,H3=2,H4=10,H5=10)\n",
    "state='./feature_extraction/models/fc/env_dvn-91.pt'\n",
    "env_model_dvn.load_state_dict(torch.load(state))\n",
    "\n",
    "env_trans=tm.envTrans(30)\n",
    "\n",
    "\n",
    "# freak model stuff\n",
    "freq_model_dvn=tm.freq_model(100,50,10)\n",
    "state='./feature_extraction/models/fc/freq-dvn-75.pt'\n",
    "freq_model_dvn.load_state_dict(torch.load(state))\n",
    "freq_trans=tm.freqTrans()\n",
    "\n",
    "\n",
    "# pitch model stuff\n",
    "fc_spec_model_dvn=tm.getFCSpecModel(D_in=20*20,H1=20,H2=80,H3=50,H4=20,H5=10)\n",
    "state='./feature_extraction/models/fc/spec_fc_dvn-97.pt'\n",
    "fc_spec_model_dvn.load_state_dict(torch.load(state))\n",
    "fc_spec_trans_dvn=tm.pitchTrans(20)\n",
    "\n",
    "# CNN_dvn model stuff\n",
    "cnn_model_dvn=tm.CNN_dvn()\n",
    "state='./feature_extraction/models/cnn/spec_dvn-99.pt'\n",
    "cnn_model_dvn.load_state_dict(torch.load(state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 18]) torch.Size([1, 360])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asalimi/miniconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 360], m2: [400 x 20] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:197",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-a4d2e7f5dcda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mfindDrum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;31m# while True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#     a,at,outputE,outputF,env_feats,freq_feats,outputP,pitch_feats=findDrum()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-a4d2e7f5dcda>\u001b[0m in \u001b[0;36mfindDrum\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv_model_dvn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mfreq_model_dvn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv_feats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfreq_feats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mfc_spec_model_dvn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpitch_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             ,pitch_feats)\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 360], m2: [400 x 20] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:197"
     ]
    }
   ],
   "source": [
    "from IPython.display import Audio\n",
    "import librosa.display\n",
    "# def findDrum():\n",
    "def randomGen():\n",
    "    out,params= stackMaker(2)\n",
    "    \n",
    "    a= memToAud(out)\n",
    "#     nz=np.max((SR-a.shape[0],0))\n",
    "#     a=np.concatenate([a[0:SR],np.zeros(nz)])\n",
    "\n",
    "#     a=librosa.core.resample(a, 44100, 47999)\n",
    "#     print(a.shape)\n",
    "#     for p in params:\n",
    "#         print(p.__dict__)\n",
    "#     if a.shape[0]>SR:\n",
    "#         print(\"a is not fine\",a,a.shape)\n",
    "#         return a\n",
    "#     print(\"a is fine\",a,a.shape)\n",
    "    return a\n",
    "\n",
    "a=randomGen()\n",
    "\n",
    "\n",
    "def findDrum():\n",
    "    at=a\n",
    "    transform_input= {\"signal\":torch.tensor(a,dtype=torch.float),\"label\":torch.tensor(0)}\n",
    "    env_feats=env_trans.call(transform_input)[\"feats\"]\n",
    "    freq_feats=freq_trans.call(transform_input)[\"feats\"]\n",
    "    pitch_feats=fc_spec_trans_dvn.call(transform_input)[\"feats\"]\n",
    "\n",
    "    print(pitch_feats.shape,torch.flatten(pitch_feats, start_dim=1).shape)\n",
    "    return (a,\n",
    "            at,env_model_dvn(env_feats),\n",
    "            freq_model_dvn(freq_feats),env_feats,freq_feats,\n",
    "            fc_spec_model_dvn(torch.flatten(pitch_feats, start_dim=1))\n",
    "            ,pitch_feats)\n",
    "\n",
    "    fig = plt.figure(figsize=(12,3))\n",
    "\n",
    "findDrum()\n",
    "# while True:\n",
    "#     a,at,outputE,outputF,env_feats,freq_feats,outputP,pitch_feats=findDrum() \n",
    "\n",
    "#     if (outputE[0]>0.6 and outputF[0]>0.6 and outputP[0]>0.6) or (outputE[0]+outputF[0]+outputP[0]>2):\n",
    "#         break\n",
    "\n",
    " \n",
    "# print(outputE,outputF,outputP)\n",
    "# ax = plt.subplot(1, 3,1)\n",
    "# plt.plot(env_feats)\n",
    "# ax = plt.subplot(1, 3,2)\n",
    "# plt.plot(freq_feats)\n",
    "# ax=plt.subplot(1,3,3)\n",
    "# librosa.display.specshow(pitch_feats[0].numpy())\n",
    "# print(a.shape)\n",
    "# Audio(a,rate=sr, autoplay=True)\n",
    "# librosa.display.waveplot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(a[0:24000],rate=sr, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-ebd9d6fcb6ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma_feats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfindDrum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# get the image for that audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#     a,num_trimmed=cutAudio(a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-ebd9d6fcb6ea>\u001b[0m in \u001b[0;36mfindDrum\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtransform_input\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"signal\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0ma_feats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"feats\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma_feats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env_model' is not defined"
     ]
    }
   ],
   "source": [
    "def findDrum():\n",
    "    out,params= stackMaker(8)\n",
    "    a= memToAud(out)\n",
    "\n",
    "    at=a[0:SR//4]\n",
    "    transform_input= {\"signal\":torch.tensor(a,dtype=torch.float),\"label\":torch.tensor(0)}\n",
    "    a_feats=env_trans.call(transform_input)[\"feats\"]\n",
    "    return a,env_model(a_feats),a_feats\n",
    "\n",
    "while True:\n",
    "    a,output,a_feats=findDrum()    \n",
    "    # get the image for that audio\n",
    "#     a,num_trimmed=cutAudio(a)\n",
    "#     im=audToImage(a,num_bins=NUM_BINS)\n",
    "#     im=librosa.util.normalize(im)\n",
    "#     plt.imshow(im)\n",
    "    \n",
    "    if output[0]>0.7:\n",
    "        break\n",
    "\n",
    "print(output)\n",
    "plt.plot(a_feats.numpy())\n",
    "Audio(a[0:15000],rate=sr, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mmakeRowSlow\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from feature_extraction import mir_utils\n",
    "# make a row of data and show what's going on\n",
    "\n",
    "def makeRowSlow():\n",
    "    ## function that makes a row of parameters and the scores for the parameters \n",
    "    ## this row can then be added to a dataframe/csv file etc\n",
    "    out,params= stackMaker(3)\n",
    "    a= memToAud(out)\n",
    "   \n",
    "    a,num_trimmed=cutAudio(a)\n",
    "    \n",
    "    dimg=z.to(device)\n",
    "    h = model.init_hidden(0,device)\n",
    "    h2 = tuple([e.data for e in h])\n",
    "    outputs,h=model(dimg,h2)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    o=outputs.cpu().detach().numpy()[0]\n",
    "    \n",
    "    #normalize scores\n",
    "    o_norm=o-min(o)\n",
    "    o_norm=o_norm/sum(o_norm)\n",
    "    score_dict=dict(zip(classes,o_norm))\n",
    "    #ranks based on score\n",
    "    ranks=1+len(classes_ranked)-ss.rankdata(o_norm) \n",
    "    rank_dict=dict(zip(classes_ranked,ranks))\n",
    "    df=pd.concat([pd.DataFrame.from_dict([rank_dict]),pd.DataFrame.from_dict([score_dict]), paramToDF(params)],axis=1) \n",
    "#     print(sorted(score_dict.items(), key=lambda x: x[1],reverse=True))\n",
    "\n",
    "    return a,score_dict,params,df,z\n",
    "counter=0\n",
    "while True:\n",
    "    counter=counter+1\n",
    "    a,score_dict,params,df,image=makeRowSlow()\n",
    "    s=sorted(score_dict.items(), key=lambda x: x[1],reverse=True)\n",
    "    if s[0][0]!=\"stacks\":\n",
    "        print(s)\n",
    "        print(\"after counts\",counter)\n",
    "        break\n",
    "    \n",
    "# librosa.display.waveplot(a)\n",
    "\n",
    "Audio(a,rate=sr, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### here's what a row looks like, next print rows like this without the headers so we can redirect it to a csv/text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['clap_rank', 'hat_rank', 'kick_rank', 'snare_rank', 'stacks_rank',\n",
       "       'clap', 'hat', 'kick', 'snare', 'stacks', 'oscType_0', 'isNoise_0',\n",
       "       'A_0', 'D_0', 'S_0', 'R_0', 'pitch_0_0', 'pitch_1_0', 'pitch_2_0',\n",
       "       'pitch_3_0', 'bpCutLow_0', 'bpCutHigh_0', 'bpOrder_0', 'amplitude_0',\n",
       "       'start_0', 'length_0', 'oscType_1', 'isNoise_1', 'A_1', 'D_1', 'S_1',\n",
       "       'R_1', 'pitch_0_1', 'pitch_1_1', 'pitch_2_1', 'pitch_3_1', 'bpCutLow_1',\n",
       "       'bpCutHigh_1', 'bpOrder_1', 'amplitude_1', 'start_1', 'length_1',\n",
       "       'oscType_2', 'isNoise_2', 'A_2', 'D_2', 'S_2', 'R_2', 'pitch_0_2',\n",
       "       'pitch_1_2', 'pitch_2_2', 'pitch_3_2', 'bpCutLow_2', 'bpCutHigh_2',\n",
       "       'bpOrder_2', 'amplitude_2', 'start_2', 'length_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from feature_extraction.mir_utils import audToImage\n",
    "from helpers import *\n",
    "\n",
    "# make a lot of rows, this can be run as its own script if you want to do multi-processing\n",
    "for i in range(100):\n",
    "    ## function that makes a row of parameters and the scores for the parameters \n",
    "    ## this row can then be added to a dataframe/csv file etc\n",
    "    out,params=stackMaker(1)\n",
    "    a=memToAud(out)\n",
    "    # get the image for that audio\n",
    "    try:\n",
    "        im=mu.audToImage(a,128)\n",
    "    except:\n",
    "        pass\n",
    "    z=librosa.util.normalize(im)\n",
    "\n",
    "    #normalize array->pilform ->apply transoforms,\n",
    "    z=(((z - z.min()) / (z.max() - z.min())) * 255.9).astype(np.uint8)\n",
    "    zi=Image.fromarray(z)\n",
    "    z=t(zi)\n",
    "    images=z.reshape([1,1,120,100])\n",
    "\n",
    "    dimg=images.to(device)\n",
    "    outputs=model(dimg,)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    o=outputs.cpu().detach().numpy()[0]\n",
    "    o_norm=o-min(o)\n",
    "    o_norm=o_norm/sum(o_norm)\n",
    "    score_dict=dict(zip(classes,o_norm))\n",
    "    #ranks based on score\n",
    "    ranks=1+len(classes_ranked)-ss.rankdata(o_norm) \n",
    "    rank_dict=dict(zip(classes_ranked,ranks))\n",
    "    df=pd.concat([pd.DataFrame.from_dict([rank_dict]),pd.DataFrame.from_dict([score_dict]),paramToDF(params)],axis=1)    \n",
    "    x=df.to_string(header=False,\n",
    "                  index=False,\n",
    "                  index_names=False).split('\\n')\n",
    "    vals = [','.join(ele.split()) for ele in x]\n",
    "    \n",
    "    print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
